{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Object Detection using the RecordIO format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes in an image as input and provides a bounding box on the image where a object of interest is along with identifying what object the box encapsulates. But before we have this solution, we need to acquire and process a traning dataset, create and setup a training job for the alorithm so that the aglorithm can learn about the dataset and then host the algorithm as an endpoint, to which we can supply the query image.\n",
    "\n",
    "This notebook is an end-to-end example introducing the Amazon SageMaker Object Detection algorithm. In this demo, we will demonstrate how to train and to host an object detection model on the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) using the Single Shot multibox Detector ([SSD](https://arxiv.org/abs/1512.02325)) algorithm. In doing so, we will also demonstrate how to construct a training dataset using the RecordIO format as this is the format that the training job will consume. We will also demonstrate how to host and validate this trained model. Amazon SageMaker Object Detection also allow training with the image and JSON format, which is illustrated in the [image and JSON Notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_image_json_format.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the Object Detection algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3 will automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::507922848584:role/service-role/AmazonSageMaker-ExecutionRole-20191025T081132\n",
      "CPU times: user 870 ms, sys: 165 ms, total: 1.03 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that you want to use for training and to store the tranied model artifacts. In this notebook, we require a custom bucket that exists so as to keep the naming clean. You can end up using a default bucket that SageMaker comes with as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket = \"aws-ml-demo-2020\" # custom bucket name.\n",
    "prefix = \"DEMO-ObjectDetection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the Amazon SageMaker Object Detection docker image, which is static and need not be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/object-detection:1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris \n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "training_image = image_uris.retrieve('object-detection',region_name)\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) was a popular computer vision challenge and they released annual challenge datasets for object detection from 2005 to 2012. In this notebook, we will use the data sets from 2007 and 2012, named as VOC07 and VOC12 respectively. Cumulatively, we have more than 20,000 images containing about 50,000 annotated objects. These annotated objects are grouped into 20 categories.\n",
    "\n",
    "While using the Pascal VOC dateset, please be aware of the database usage rights:\n",
    "\"The VOC data includes images obtained from the \"flickr\" website. Use of these images must respect the corresponding terms of use: \n",
    "* \"flickr\" terms of use (https://www.flickr.com/help/terms)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Let us download the Pascal VOC datasets from 2007 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Download the dataset\n",
    "!wget -P /tmp http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\n",
    "!wget -P /tmp http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
    "!wget -P /tmp http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "# # Extract the data.\n",
    "!tar -xf /tmp/VOCtrainval_11-May-2012.tar && rm /tmp/VOCtrainval_11-May-2012.tar\n",
    "!tar -xf /tmp/VOCtrainval_06-Nov-2007.tar && rm /tmp/VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf /tmp/VOCtest_06-Nov-2007.tar && rm /tmp/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC2007  VOC2012\r\n"
     ]
    }
   ],
   "source": [
    "!ls VOCdevkit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into RecordIO\n",
    "[RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) is a highly efficient binary data format from [MXNet](https://mxnet.incubator.apache.org/) that makes it easy and simple to prepare the dataset and transfer to the instance that will run the training job. To generate a RecordIO file, we will use the tools from MXNet. The provided tools will first generate a list file and then use the [im2rec tool](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py) to create the [RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) file. More details on how to generate RecordIO file for object detection task, see the [MXNet example](https://github.com/apache/incubator-mxnet/tree/master/example/ssd).\n",
    "\n",
    "We will combine the training and validation sets from both 2007 and 2012 as the training data set, and use the test set from 2007 as our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving list to disk...\n",
      "List file VOCdevkit/train.lst generated...\n",
      "Creating .rec file from /home/ec2-user/SageMaker/object_detection_pascalvoc_coco_2020-11-24/VOCdevkit/train.lst in /home/ec2-user/SageMaker/object_detection_pascalvoc_coco_2020-11-24/VOCdevkit\n",
      "time: 0.048779964447021484  count: 0\n",
      "time: 2.0392513275146484  count: 1000\n",
      "time: 2.0881295204162598  count: 2000\n",
      "time: 1.9781060218811035  count: 3000\n",
      "time: 2.033107042312622  count: 4000\n",
      "time: 2.014026403427124  count: 5000\n",
      "time: 2.0233044624328613  count: 6000\n",
      "time: 2.0468826293945312  count: 7000\n",
      "time: 2.016981601715088  count: 8000\n",
      "time: 2.021055221557617  count: 9000\n",
      "time: 2.0359718799591064  count: 10000\n",
      "time: 2.0449962615966797  count: 11000\n",
      "time: 1.9941303730010986  count: 12000\n",
      "time: 2.031057834625244  count: 13000\n",
      "time: 2.030531644821167  count: 14000\n",
      "time: 2.0220155715942383  count: 15000\n",
      "time: 2.164947748184204  count: 16000\n",
      "Record file VOCdevkit/train.rec generated...\n",
      "saving list to disk...\n",
      "List file VOCdevkit/val.lst generated...\n",
      "Creating .rec file from /home/ec2-user/SageMaker/object_detection_pascalvoc_coco_2020-11-24/VOCdevkit/val.lst in /home/ec2-user/SageMaker/object_detection_pascalvoc_coco_2020-11-24/VOCdevkit\n",
      "time: 0.011272192001342773  count: 0\n",
      "time: 1.8666739463806152  count: 1000\n",
      "time: 1.8762493133544922  count: 2000\n",
      "time: 1.8637728691101074  count: 3000\n",
      "time: 1.8899335861206055  count: 4000\n",
      "Record file VOCdevkit/val.rec generated...\n"
     ]
    }
   ],
   "source": [
    "!python tools/prepare_dataset.py --dataset pascal --year 2007,2012 --set trainval --target VOCdevkit/train.lst\n",
    "# !rm -rf VOCdevkit/VOC2012\n",
    "!python tools/prepare_dataset.py --dataset pascal --year 2007 --set test --target VOCdevkit/val.lst --no-shuffle\n",
    "# !rm -rf VOCdevkit/VOC2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "Upload the data to the S3 bucket. We do this in multiple channels. Channels are simply directories in the bucket that differentiate between training and validation data. Let us simply call these directories `train` and `validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aws-ml-demo-2020\" # custom bucket name.\n",
    "prefix = \"DEMO-ObjectDetection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 15.3 s, total: 46.4 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Upload the RecordIO files to train and validation channels\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='VOCdevkit/train.rec', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='VOCdevkit/val.rec', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
